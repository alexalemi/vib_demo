{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX_DVIB_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4PYHO4b-W00"
      },
      "source": [
        "# JAX Implementation of Deep Variational Information Bottleneck\r\n",
        "\r\n",
        "This notebook serves as a modern JAX remake of the code that powered the [Deep Variational Information Bottleneck](https://arxiv.org/abs/1612.00410) paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUDZcsyg-Nd0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agTvc877FUSG"
      },
      "source": [
        "I'd recommend using a GPU kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v59oMwFiRy_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "8d1b3e13-ccdc-4b0f-fd47-d0f8a0ad8456"
      },
      "source": [
        "#@title requirements\r\n",
        "!pip install flax"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from flax) (1.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from flax) (1.19.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from flax) (0.8)\n",
            "Requirement already satisfied: jax>=0.2.6 in /usr/local/lib/python3.6/dist-packages (from flax) (0.2.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flax) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flax) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flax) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flax) (0.10.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax>=0.2.6->flax) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax>=0.2.6->flax) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->flax) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7ohB682HW3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "5c57505a-093e-4ce8-eb29-9d42a878f6e9"
      },
      "source": [
        "#@title imports\r\n",
        "import functools\r\n",
        "\r\n",
        "import jax\r\n",
        "import jax.numpy as np\r\n",
        "from jax import grad, vmap, jit, random\r\n",
        "from typing import Any\r\n",
        "from pprint import pprint\r\n",
        "\r\n",
        "import flax\r\n",
        "import flax.linen as nn\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.style.use('default')\r\n",
        "\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "\r\n",
        "print(\"JAX Devices: \", jax.devices())"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "JAX Devices:  [GpuDevice(id=0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U9-TW46I4qv",
        "cellView": "form"
      },
      "source": [
        "#@title data\r\n",
        "dataset = tfds.load('mnist', split='train').batch(60_000).cache()\r\n",
        "data = jax.device_put(next(dataset.as_numpy_iterator()))\r\n",
        "batch = jax.tree_map(lambda x: x[:100], data)\r\n",
        "\r\n",
        "eval_dataset = tfds.load('mnist', split='test').batch(10_000).cache()\r\n",
        "eval_ds = jax.device_put(next(eval_dataset.as_numpy_iterator()))"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHH1Qx0c-PXq"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0gQliwgY5Ozs"
      },
      "source": [
        "#@title Distributions\r\n",
        "\r\n",
        "@flax.struct.dataclass\r\n",
        "class MultivariateNormalDiag():\r\n",
        "  locs: np.ndarray\r\n",
        "  scales: np.ndarray\r\n",
        "\r\n",
        "  def log_prob(self, x):\r\n",
        "    return jax.scipy.stats.norm.logpdf(\r\n",
        "        x, loc=self.locs, scale=self.scales).sum(-1)\r\n",
        "  \r\n",
        "  def sample(self, rng, shape=()):\r\n",
        "    return self.locs + self.scales * random.normal(\r\n",
        "        rng, shape + self.locs.shape)\r\n",
        "\r\n",
        "@flax.struct.dataclass\r\n",
        "class Categorical():\r\n",
        "  logits: np.ndarray\r\n",
        "\r\n",
        "  def log_prob(self, x):\r\n",
        "    @functools.partial(np.vectorize, signature='(k),()->()')\r\n",
        "    def f(logits, x):\r\n",
        "      logits = jax.nn.log_softmax(logits, axis=-1)\r\n",
        "      return logits[x]\r\n",
        "    return f(self.logits, x)\r\n",
        "\r\n",
        "  def sample(self, rng, shape=()):\r\n",
        "    return random.categorical(\r\n",
        "        rng, self.logits, axis=-1, shape=shape)"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWRpJdCnHpiF",
        "cellView": "form"
      },
      "source": [
        "#@title Model\n",
        "\n",
        "kernel_init = jax.nn.initializers.xavier_uniform()\n",
        "bias_init = jax.nn.initializers.zeros\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  embedding_width: int = 256\n",
        "  nonlinearity: Any = jax.nn.relu\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    # rescale\n",
        "    x = x / 128.0 - 1.0\n",
        "    x = x.reshape((-1, 28 * 28))\n",
        "    x = self.nonlinearity(\n",
        "        nn.Dense(1024,\n",
        "                 kernel_init=kernel_init,\n",
        "                 bias_init=bias_init)(x))\n",
        "    x = self.nonlinearity(\n",
        "        nn.Dense(1024,\n",
        "                 kernel_init=kernel_init,\n",
        "                 bias_init=bias_init)(x))\n",
        "    means = nn.Dense(\n",
        "        self.embedding_width,\n",
        "        kernel_init=kernel_init,\n",
        "        bias_init=bias_init)(x)\n",
        "    rhos = nn.Dense(\n",
        "        self.embedding_width,\n",
        "        kernel_init=kernel_init,\n",
        "        bias_init=bias_init)(x)\n",
        "\n",
        "    return MultivariateNormalDiag(\n",
        "        means, jax.nn.softplus(rhos - 5.0))\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  classes: int = 10\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, z):\n",
        "    logits = nn.Dense(\n",
        "        self.classes,\n",
        "        kernel_init=kernel_init,\n",
        "        bias_init=bias_init)(z)\n",
        "    return Categorical(logits=logits)\n",
        "\n",
        "bits = np.log(2)\n",
        "\n",
        "class VIB(nn.Module):\n",
        "  width: int = 256\n",
        "  num_samples: int = 16\n",
        "  num_classes: int = 10\n",
        "  beta: float = 1e-3\n",
        "\n",
        "  def setup(self):\n",
        "    self.encoder = Encoder(self.width)\n",
        "    self.decoder = Decoder(self.num_classes)\n",
        "    self.prior = MultivariateNormalDiag(\n",
        "        np.zeros(self.width), np.ones(self.width))\n",
        "\n",
        "  def __call__(self, batch, rng):\n",
        "    image = batch['image']\n",
        "    z_dist = self.encoder(image)\n",
        "    z_samples = z_dist.sample(rng, (self.num_samples,))\n",
        "    pred_dist = self.decoder(z_samples)\n",
        "\n",
        "    class_loss = -pred_dist.log_prob(batch['label']) / bits\n",
        "    rate = (z_dist.log_prob(z_samples) -\n",
        "            self.prior.log_prob(z_samples)) / bits\n",
        "    loss = class_loss + self.beta * rate\n",
        "\n",
        "    # metrics \n",
        "    err = 1-(pred_dist.logits.argmax(-1) == batch['label']).mean()\n",
        "    avg_logits = jax.nn.logsumexp(\n",
        "        jax.nn.log_softmax(pred_dist.logits, axis=-1), axis=0)\n",
        "    avg_err = 1-(avg_logits.argmax(-1) == batch['label']).mean()\n",
        "    avg_loss = -Categorical(logits=avg_logits).log_prob(\n",
        "        batch['label']) / bits\n",
        "\n",
        "    return loss.mean(), {\n",
        "        'c': class_loss,\n",
        "        'r': rate,\n",
        "        'loss': loss,\n",
        "        'err': err,\n",
        "        'avg_err': avg_err,\n",
        "        'avg_loss': avg_loss\n",
        "    }"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXjtAxFCJ-ec",
        "cellView": "form"
      },
      "source": [
        "#@title Training\n",
        "\n",
        "@jax.jit\n",
        "def train_step(optimizer, params_ema, batch, rng, learning_rate):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "\n",
        "  def loss_fn(params):\n",
        "    return vib.apply(params, batch, rng)\n",
        "\n",
        "  (loss, aux), grad = jax.value_and_grad(\n",
        "      loss_fn, has_aux=True)(\n",
        "          optimizer.target)\n",
        "  optimizer = optimizer.apply_gradient(\n",
        "      grad, learning_rate=learning_rate)\n",
        "  params_ema = jax.tree_multimap(\n",
        "      lambda p_ema, p: p_ema * 0.999 + p * 0.001,\n",
        "      params_ema, optimizer.target)\n",
        "  return optimizer, params_ema, aux\n",
        "\n",
        "reshaped_eval_ds = {\n",
        "    'image': eval_ds['image'].reshape(\n",
        "        (100, -1, 28, 28, 1)),\n",
        "    'label': eval_ds['label'].reshape(\n",
        "        (100, -1))\n",
        "}\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def evaluate(params, dataset, rng):\n",
        "\n",
        "  def f(part):\n",
        "    _, aux = vib.clone(num_samples=1024).apply(\n",
        "        params, part, rng)\n",
        "    return aux\n",
        "\n",
        "  return jax.tree_map(np.mean, jax.lax.map(f, dataset))\n",
        "\n",
        "def epoch(optimizer,\n",
        "          params_ema,\n",
        "          data,\n",
        "          rng,\n",
        "          learning_rate,\n",
        "          eval_rng=None,\n",
        "          batch_size=100):\n",
        "  if eval_rng is None:\n",
        "    eval_rng = random.PRNGKey(0)\n",
        "  steps_per_epoch = len(data['image']) // batch_size\n",
        "\n",
        "  rng, spl = random.split(rng)\n",
        "  perms = random.permutation(spl, len(data['image']))\n",
        "  perms = perms[:steps_per_epoch * batch_size]\n",
        "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "  @jax.jit\n",
        "  def segment(state, perm):\n",
        "    (optimizer, params_ema, rng) = state\n",
        "    rng, spl = random.split(rng)\n",
        "    batch = {k: v[perm, ...] for k, v in data.items()}\n",
        "    optimizer, params_ema, aux = train_step(\n",
        "        optimizer, params_ema, batch, spl, learning_rate)\n",
        "    return (optimizer, params_ema,\n",
        "            rng), jax.tree_map(np.mean, aux)\n",
        "\n",
        "  (optimizer, params_ema,\n",
        "   rng), batch_stats = jax.lax.scan(\n",
        "       segment, (optimizer, params_ema, rng), perms)\n",
        "\n",
        "  eval_aux = evaluate(params_ema,\n",
        "                      reshaped_eval_ds,\n",
        "                      eval_rng)\n",
        "\n",
        "  return (optimizer, params_ema, spl,\n",
        "          batch_stats, eval_aux)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vwblgr_-Qw-"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PajbQRUoIceX",
        "cellView": "form"
      },
      "source": [
        "#@title Init\n",
        "seed = 32828\n",
        "rng = random.PRNGKey(seed)\n",
        "rng, init_rng, eval_rng = random.split(rng, 3)\n",
        "vib = VIB(num_samples=12, beta=1e-3)\n",
        "\n",
        "params = vib.init(init_rng, batch, rng)\n",
        "params_ema = vib.init(init_rng, batch, rng)\n",
        "\n",
        "optimizer_def = flax.optim.Adam()\n",
        "optimizer = optimizer_def.create(params)\n",
        "learning_rate = 2e-4"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eXdb0CiCzoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d12b7be3-b3d4-496b-9d98-6832c90a1c63"
      },
      "source": [
        "#@title run\n",
        "prep = lambda x: f\"{float(np.mean(x)):.4}\"\n",
        "\n",
        "counter = 0\n",
        "eval_stats = []\n",
        "for i in range(25):\n",
        "  optimizer, params_ema, rng, batch_stats, eval_aux = epoch(\n",
        "      optimizer, params_ema, data, rng, learning_rate, eval_rng)\n",
        "  counter += 1\n",
        "  if (counter > 0) and (counter % 2 == 0):\n",
        "    learning_rate *= 0.95 # 0.97\n",
        "  print(counter, flush=True)\n",
        "  eval_stats.append(jax.tree_map(lambda x: float(np.mean(x)), eval_aux))\n",
        "  print(\"TRAIN:\", jax.tree_map(prep, batch_stats), flush=True)\n",
        "  print(\"EVAL: \", jax.tree_map(prep, eval_aux), flush=True)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "TRAIN: {'avg_err': '0.0807', 'avg_loss': '0.3949', 'c': '0.4444', 'err': '0.0946', 'loss': '0.6215', 'r': '177.1'}\n",
            "EVAL:  {'avg_err': '0.1439', 'avg_loss': '0.9107', 'c': '0.9112', 'err': '0.1441', 'loss': '2.041', 'r': '1.13e+03'}\n",
            "2\n",
            "TRAIN: {'avg_err': '0.03227', 'avg_loss': '0.165', 'c': '0.1937', 'err': '0.04085', 'loss': '0.2817', 'r': '88.05'}\n",
            "EVAL:  {'avg_err': '0.0464', 'avg_loss': '0.2713', 'c': '0.2731', 'err': '0.0478', 'loss': '0.8839', 'r': '610.8'}\n",
            "3\n",
            "TRAIN: {'avg_err': '0.02038', 'avg_loss': '0.1096', 'c': '0.1291', 'err': '0.02718', 'loss': '0.2082', 'r': '79.1'}\n",
            "EVAL:  {'avg_err': '0.0282', 'avg_loss': '0.1419', 'c': '0.1465', 'err': '0.03004', 'loss': '0.4569', 'r': '310.4'}\n",
            "4\n",
            "TRAIN: {'avg_err': '0.01583', 'avg_loss': '0.08485', 'c': '0.1014', 'err': '0.02144', 'loss': '0.1741', 'r': '72.67'}\n",
            "EVAL:  {'avg_err': '0.0208', 'avg_loss': '0.1014', 'c': '0.109', 'err': '0.02357', 'loss': '0.2834', 'r': '174.3'}\n",
            "5\n",
            "TRAIN: {'avg_err': '0.01018', 'avg_loss': '0.06078', 'c': '0.07337', 'err': '0.01527', 'loss': '0.1401', 'r': '66.76'}\n",
            "EVAL:  {'avg_err': '0.0175', 'avg_loss': '0.08358', 'c': '0.09361', 'err': '0.0203', 'loss': '0.2091', 'r': '115.5'}\n",
            "6\n",
            "TRAIN: {'avg_err': '0.007683', 'avg_loss': '0.04857', 'c': '0.05886', 'err': '0.01231', 'loss': '0.1222', 'r': '63.31'}\n",
            "EVAL:  {'avg_err': '0.016', 'avg_loss': '0.07384', 'c': '0.08534', 'err': '0.01841', 'loss': '0.1767', 'r': '91.33'}\n",
            "7\n",
            "TRAIN: {'avg_err': '0.004833', 'avg_loss': '0.03664', 'c': '0.0449', 'err': '0.009169', 'loss': '0.1036', 'r': '58.73'}\n",
            "EVAL:  {'avg_err': '0.0153', 'avg_loss': '0.06848', 'c': '0.08145', 'err': '0.01728', 'loss': '0.1592', 'r': '77.72'}\n",
            "8\n",
            "TRAIN: {'avg_err': '0.00415', 'avg_loss': '0.032', 'c': '0.03957', 'err': '0.008079', 'loss': '0.09575', 'r': '56.18'}\n",
            "EVAL:  {'avg_err': '0.0142', 'avg_loss': '0.06504', 'c': '0.07899', 'err': '0.01659', 'loss': '0.1501', 'r': '71.06'}\n",
            "9\n",
            "TRAIN: {'avg_err': '0.003117', 'avg_loss': '0.02596', 'c': '0.03238', 'err': '0.006612', 'loss': '0.08541', 'r': '53.03'}\n",
            "EVAL:  {'avg_err': '0.0137', 'avg_loss': '0.06221', 'c': '0.07707', 'err': '0.01589', 'loss': '0.1433', 'r': '66.19'}\n",
            "10\n",
            "TRAIN: {'avg_err': '0.001917', 'avg_loss': '0.0207', 'c': '0.02577', 'err': '0.005054', 'loss': '0.07504', 'r': '49.27'}\n",
            "EVAL:  {'avg_err': '0.0134', 'avg_loss': '0.06067', 'c': '0.07694', 'err': '0.01556', 'loss': '0.1374', 'r': '60.42'}\n",
            "11\n",
            "TRAIN: {'avg_err': '0.001983', 'avg_loss': '0.01913', 'c': '0.02434', 'err': '0.004764', 'loss': '0.07209', 'r': '47.75'}\n",
            "EVAL:  {'avg_err': '0.0136', 'avg_loss': '0.06024', 'c': '0.07802', 'err': '0.01544', 'loss': '0.135', 'r': '56.98'}\n",
            "12\n",
            "TRAIN: {'avg_err': '0.001333', 'avg_loss': '0.01593', 'c': '0.0202', 'err': '0.003736', 'loss': '0.06433', 'r': '44.13'}\n",
            "EVAL:  {'avg_err': '0.0132', 'avg_loss': '0.05953', 'c': '0.07893', 'err': '0.01532', 'loss': '0.1318', 'r': '52.89'}\n",
            "13\n",
            "TRAIN: {'avg_err': '0.00075', 'avg_loss': '0.0137', 'c': '0.01719', 'err': '0.00304', 'loss': '0.05881', 'r': '41.62'}\n",
            "EVAL:  {'avg_err': '0.0132', 'avg_loss': '0.05906', 'c': '0.08014', 'err': '0.01517', 'loss': '0.1293', 'r': '49.13'}\n",
            "14\n",
            "TRAIN: {'avg_err': '0.001133', 'avg_loss': '0.01357', 'c': '0.01759', 'err': '0.003324', 'loss': '0.0593', 'r': '41.71'}\n",
            "EVAL:  {'avg_err': '0.013', 'avg_loss': '0.05911', 'c': '0.08152', 'err': '0.01508', 'loss': '0.1291', 'r': '47.57'}\n",
            "15\n",
            "TRAIN: {'avg_err': '0.00175', 'avg_loss': '0.01556', 'c': '0.02042', 'err': '0.004183', 'loss': '0.06247', 'r': '42.06'}\n",
            "EVAL:  {'avg_err': '0.0135', 'avg_loss': '0.05964', 'c': '0.08303', 'err': '0.01495', 'loss': '0.1299', 'r': '46.84'}\n",
            "16\n",
            "TRAIN: {'avg_err': '0.0002667', 'avg_loss': '0.007985', 'c': '0.009945', 'err': '0.001511', 'loss': '0.0454', 'r': '35.45'}\n",
            "EVAL:  {'avg_err': '0.0129', 'avg_loss': '0.05883', 'c': '0.08485', 'err': '0.01498', 'loss': '0.1264', 'r': '41.52'}\n",
            "17\n",
            "TRAIN: {'avg_err': '0.0002167', 'avg_loss': '0.007416', 'c': '0.009268', 'err': '0.001407', 'loss': '0.04317', 'r': '33.9'}\n",
            "EVAL:  {'avg_err': '0.0126', 'avg_loss': '0.05821', 'c': '0.08662', 'err': '0.01505', 'loss': '0.1248', 'r': '38.21'}\n",
            "18\n",
            "TRAIN: {'avg_err': '0.0004333', 'avg_loss': '0.00878', 'c': '0.01142', 'err': '0.001926', 'loss': '0.04642', 'r': '35.0'}\n",
            "EVAL:  {'avg_err': '0.0124', 'avg_loss': '0.05828', 'c': '0.08832', 'err': '0.01502', 'loss': '0.1255', 'r': '37.19'}\n",
            "19\n",
            "TRAIN: {'avg_err': '0.0018', 'avg_loss': '0.01513', 'c': '0.02127', 'err': '0.00466', 'loss': '0.06208', 'r': '40.81'}\n",
            "EVAL:  {'avg_err': '0.0124', 'avg_loss': '0.05889', 'c': '0.08802', 'err': '0.01477', 'loss': '0.1286', 'r': '40.58'}\n",
            "20\n",
            "TRAIN: {'avg_err': '0.0001167', 'avg_loss': '0.005596', 'c': '0.006872', 'err': '0.0009667', 'loss': '0.03818', 'r': '31.31'}\n",
            "EVAL:  {'avg_err': '0.0127', 'avg_loss': '0.05848', 'c': '0.09063', 'err': '0.01511', 'loss': '0.1266', 'r': '35.98'}\n",
            "21\n",
            "TRAIN: {'avg_err': '5e-05', 'avg_loss': '0.005372', 'c': '0.006672', 'err': '0.0009556', 'loss': '0.03655', 'r': '29.88'}\n",
            "EVAL:  {'avg_err': '0.0123', 'avg_loss': '0.05727', 'c': '0.09157', 'err': '0.01522', 'loss': '0.1245', 'r': '32.89'}\n",
            "22\n",
            "TRAIN: {'avg_err': '0.00025', 'avg_loss': '0.006927', 'c': '0.009041', 'err': '0.001511', 'loss': '0.04019', 'r': '31.15'}\n",
            "EVAL:  {'avg_err': '0.0123', 'avg_loss': '0.05681', 'c': '0.0921', 'err': '0.01506', 'loss': '0.1244', 'r': '32.27'}\n",
            "23\n",
            "TRAIN: {'avg_err': '0.0005167', 'avg_loss': '0.006989', 'c': '0.009365', 'err': '0.001622', 'loss': '0.03999', 'r': '30.62'}\n",
            "EVAL:  {'avg_err': '0.0122', 'avg_loss': '0.05676', 'c': '0.09346', 'err': '0.01503', 'loss': '0.125', 'r': '31.5'}\n",
            "24\n",
            "TRAIN: {'avg_err': '0.00095', 'avg_loss': '0.00982', 'c': '0.01428', 'err': '0.002858', 'loss': '0.04883', 'r': '34.55'}\n",
            "EVAL:  {'avg_err': '0.012', 'avg_loss': '0.05783', 'c': '0.09439', 'err': '0.01484', 'loss': '0.1278', 'r': '33.44'}\n",
            "25\n",
            "TRAIN: {'avg_err': '0.00015', 'avg_loss': '0.005221', 'c': '0.006716', 'err': '0.001024', 'loss': '0.03576', 'r': '29.05'}\n",
            "EVAL:  {'avg_err': '0.0119', 'avg_loss': '0.05788', 'c': '0.09684', 'err': '0.01508', 'loss': '0.1279', 'r': '31.08'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwXU51ILWQ0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720aab58-aee1-45e8-8cdd-b44ad9077e71"
      },
      "source": [
        "# final evaluation\r\n",
        "result = evaluate(params_ema, eval_ds, eval_rng)\r\n",
        "print(result)\r\n",
        "print(f\"{result['avg_err']:.2%}\\t{result['avg_loss']:.4}\")"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'avg_err': DeviceArray(0.0118, dtype=float32), 'avg_loss': DeviceArray(0.05816593, dtype=float32), 'c': DeviceArray(0.09675445, dtype=float32), 'err': DeviceArray(0.01501162, dtype=float32), 'loss': DeviceArray(0.12789465, dtype=float32), 'r': DeviceArray(31.140131, dtype=float32)}\n",
            "1.18%\t0.05817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yUFCUvMGsGO"
      },
      "source": [
        ""
      ],
      "execution_count": 230,
      "outputs": []
    }
  ]
}